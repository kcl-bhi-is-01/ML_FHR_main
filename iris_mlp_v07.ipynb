{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56edd0ae",
   "metadata": {},
   "source": [
    "## Iris homebrew network ##\n",
    "\n",
    "This notebook uses the renown **iris** data set to solve a 3 class classification problem, using a Multi Layer Perceptron with 1 hidden layer and an output layer of 3 *one-hotted* linear units feeding into a *softmax* function. \n",
    "\n",
    "The Loss function is *Multi-class Cross Entropy*\n",
    "\n",
    "You can use the cell below to select either *ReLU* or *sigmoid* functions, and otherwise change the network parameters\n",
    "\n",
    "*What differences in performance do you notice between ReLU and sigmoid?*\n",
    "\n",
    "*Why do you think this is?*\n",
    "\n",
    "You can use this notebook to try out other hidden layer activation functions such as *tanh* or the variations of *ReLU*. You could also add additional hidden layers\n",
    "\n",
    "Run the notebook cells in the sequence they are given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68878b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Get data from github\n",
    "#\n",
    "!wget -nv https://github.com/kcl-bhi-is-01/datasets/raw/main/iris.csv\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61963c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Main imports and network parameters\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "#\n",
    "iris_df = pd.read_csv(\"iris.csv\")\n",
    "#\n",
    "# Parms\n",
    "#\n",
    "#hidden_layer_activation = \"ReLU\"\n",
    "hidden_layer_activation = \"sigmoid\"\n",
    "#\n",
    "hidden_layer_size = 7\n",
    "learning_rate     = 0.4\n",
    "epochs            = 100\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Functions\n",
    "#\n",
    "def sigmoid(np_matrix):\n",
    "    #\n",
    "    # Sigmoid activation\n",
    "    #\n",
    "    return 1 / (1 + np.exp(- np_matrix))\n",
    "#\n",
    "#\n",
    "def sigmoid_derivative(np_matrix):\n",
    "    #\n",
    "    # Sigmoid derivative\n",
    "    #\n",
    "    sig_matrix = sigmoid(np_matrix)\n",
    "    #\n",
    "    return np.multiply(sig_matrix, (1 - sig_matrix))\n",
    "#\n",
    "#\n",
    "def ReLU(np_matrix):\n",
    "    #\n",
    "    # ReLU activation\n",
    "    #\n",
    "    return np.maximum(np_matrix, 0)\n",
    "#\n",
    "#\n",
    "def ReLU_derivative(np_matrix):\n",
    "    #\n",
    "    # ReLU derivative\n",
    "    #\n",
    "    return np.array(np_matrix == 1).astype(float)\n",
    "#\n",
    "#\n",
    "def softmax(np_vector):\n",
    "    #\n",
    "    # softmax\n",
    "    #\n",
    "    exp_vect = np.exp(np_vector)\n",
    "    denom    = np.sum(exp_vect)\n",
    "    #\n",
    "    return exp_vect / denom\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Split\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "Y = list(iris_df[\"class\"])\n",
    "X = iris_df.drop(\"class\", axis=1)\n",
    "#\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Fit and apply standard scaling\n",
    "#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#\n",
    "scaler = StandardScaler() \n",
    "#\n",
    "scaler.fit(X_train)\n",
    "#\n",
    "X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "X_test  = pd.DataFrame(scaler.transform(X_test))\n",
    "#\n",
    "# Reapply column names\n",
    "#\n",
    "X_train.columns = X.columns\n",
    "X_test.columns  = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# One hot encoding\n",
    "#\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#\n",
    "enc = OneHotEncoder()\n",
    "#\n",
    "Y_train_ar = np.array(Y_train).reshape(-1, 1)\n",
    "Y_test_ar  = np.array(Y_test).reshape(-1, 1)\n",
    "#\n",
    "enc.fit(Y_train_ar)\n",
    "#\n",
    "Y_train_oh = enc.transform(Y_train_ar).toarray()\n",
    "Y_test_oh  = enc.transform(Y_test_ar).toarray()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Set up weights input to hidden\n",
    "#\n",
    "w_i_h1 = np.random.uniform(low=-0.3, high=0.3, size=(X_train.shape[1] + 1, hidden_layer_size))\n",
    "print(w_i_h1.shape)\n",
    "#\n",
    "# Set up weights hidden to output\n",
    "#\n",
    "w_h1_o = np.random.uniform(low=-0.3, high=0.3, size=(hidden_layer_size + 1, Y_train_oh.shape[1]))\n",
    "print(w_h1_o.shape)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745746b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Set up training data with bias\n",
    "#\n",
    "X_train_np = np.concatenate((np.ones([X_train.shape[0], 1]), X_train.to_numpy()), axis=1)\n",
    "#\n",
    "train_obs = X_train_np.shape[0]\n",
    "print(X_train_np.shape)\n",
    "#\n",
    "# Get the mean of data in input layer oh no Mrs ...\n",
    "#\n",
    "#i_mean = np.mean(X_train_np, axis=0)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e20822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Train\n",
    "#\n",
    "losses = [0] * epochs\n",
    "#\n",
    "for epoch in range(epochs):\n",
    "    #\n",
    "    # Forward pass\n",
    "    #\n",
    "    h1_sum = np.matmul(X_train_np, w_i_h1)\n",
    "    #\n",
    "    # Selection of activation function\n",
    "    #\n",
    "    if hidden_layer_activation == \"sigmoid\":\n",
    "        h1_act = sigmoid(h1_sum)\n",
    "    elif hidden_layer_activation == \"ReLU\":\n",
    "        h1_act = ReLU(h1_sum)\n",
    "    #\n",
    "    # Add bias column to h1 activation matrix (i.e. bias has an activation 1)\n",
    "    #\n",
    "    h1_act = np.concatenate((np.ones([X_train_np.shape[0], 1]), h1_act), axis=1)\n",
    "    #\n",
    "    o_sum = np.matmul(h1_act, w_h1_o)\n",
    "    #\n",
    "    soft_act = np.apply_along_axis(softmax, 1, o_sum)\n",
    "    #\n",
    "    # Mean CE loss a bit more meaningful than the sum over all observations\n",
    "    #\n",
    "    loss = -np.sum(np.sum(np.multiply(Y_train_oh, np.log(soft_act)), axis=1)) / Y_train_oh.shape[0]\n",
    "    #\n",
    "    print(\"Epoch:\", epoch + 1, \" Loss: \", round(loss, 6))\n",
    "    #\n",
    "    losses[epoch] = loss\n",
    "    #\n",
    "    # Backward pass\n",
    "    #\n",
    "    # Notes: a) dl/do = softmax vector minus one-hot vector\n",
    "    #        b) gradients taken as mean across all observations\n",
    "    #        c) Update weights on each pass\n",
    "    #\n",
    "    #\n",
    "    dl_do = soft_act - Y_train_oh\n",
    "    #\n",
    "    # dl/d_w_h1_o = dl/do * do/d_w_h1_o (do/d_w_h1_o = w_h1_o since derivative of weighted sum wrt weights) \n",
    "    #\n",
    "    dl_by_d_w_h1_o = np.zeros([train_obs, w_h1_o.shape[0], w_h1_o.shape[1]])\n",
    "    #\n",
    "    dl_by_d_w_i_h1 = np.zeros([train_obs, w_i_h1.shape[0], w_i_h1.shape[1]])\n",
    "    #\n",
    "    dl_dh = np.zeros([train_obs, w_h1_o.shape[0]]) \n",
    "    #\n",
    "    # Calculate gradients and weight deltas - hidden to output\n",
    "    #\n",
    "    for i in range(train_obs):  # Each observation\n",
    "        for j in range(w_h1_o.shape[0]): # Each unit in hidden layer (including bias)\n",
    "            for k in range(w_h1_o.shape[1]): # Each unit in output layer\n",
    "                #\n",
    "                dl_dh[i, j] += w_h1_o[j, k] * dl_do[i, k] # Hidden layer gradient\n",
    "                #\n",
    "                dl_by_d_w_h1_o[i, j, k] = dl_do[i, k] * h1_act[i, j] # Weight deltas (prior to lr)\n",
    "            #\n",
    "        #\n",
    "    #\n",
    "    # Calculate weight delta - input to hidden \n",
    "    #\n",
    "    for i in range(train_obs):  # Each observation\n",
    "        for j in range(w_i_h1.shape[0]): # Each input feature (including bias)\n",
    "            for k in range(w_i_h1.shape[1]): # Each unit in hidden layer\n",
    "                #\n",
    "                dl_by_d_w_i_h1[i, j, k] = dl_dh[i, k] * X_train_np[i, j] # Weight deltas (prior to lr)\n",
    "            #\n",
    "        #\n",
    "    #\n",
    "    # Update weights\n",
    "    #\n",
    "    w_h1_o -= np.mean(dl_by_d_w_h1_o, axis=0) * learning_rate\n",
    "    w_i_h1 -= np.mean(dl_by_d_w_i_h1, axis=0 )* learning_rate\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Plot loss functions\n",
    "#\n",
    "plt.plot(losses, color=\"red\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f21075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Test - Y_test_oh\n",
    "#\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "#\n",
    "X_test_np = np.concatenate((np.ones([X_test.shape[0], 1]), X_test.to_numpy()), axis=1)\n",
    "#\n",
    "test_obs = X_test_np.shape[0]\n",
    "#\n",
    "h1_sum = np.matmul(X_test_np, w_i_h1)\n",
    "#\n",
    "h1_act = np.zeros([test_obs, hidden_layer_size])\n",
    "#\n",
    "# Selection of activation function\n",
    "#\n",
    "if hidden_layer_activation == \"sigmoid\":\n",
    "    h1_act = sigmoid(h1_sum)\n",
    "elif hidden_layer_activation == \"ReLU\":\n",
    "    h1_act = ReLU(h1_sum)\n",
    "#\n",
    "# Add bias column to h1 activation matrix (i.e. bias has an activation 1)\n",
    "#\n",
    "h1_act = np.concatenate((np.ones([X_test_np.shape[0], 1]), h1_act), axis=1)\n",
    "#\n",
    "o_sum = np.matmul(h1_act, w_h1_o)\n",
    "#\n",
    "soft_act = np.apply_along_axis(softmax, 1, o_sum)\n",
    "#\n",
    "# I use mean CE loss to be a bit more meaningful\n",
    "#\n",
    "loss = -np.sum(np.sum(np.multiply(Y_test_oh, np.log(soft_act)), axis=1)) / Y_test_oh.shape[0]\n",
    "#\n",
    "# Metrics on test\n",
    "#\n",
    "print(\"Test set Loss: \", round(loss, 6))\n",
    "#\n",
    "actual    = np.argmax(Y_test_oh, axis=1)\n",
    "predicted = np.argmax(soft_act, axis=1)\n",
    "#\n",
    "confusion = confusion_matrix(actual, predicted)\n",
    "conf_disp = ConfusionMatrixDisplay(confusion_matrix=confusion)\n",
    "conf_disp.plot()\n",
    "plt.show()\n",
    "#\n",
    "print(classification_report(actual, predicted))\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
